{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Whisper-Small Baseline Evaluation on FLEURS Hindi\n",
        "**JoshTalks ASR Research | Q1 — Baseline Evaluation**\n",
        "\n",
        "Evaluates pretrained `openai/whisper-small` on a subset of FLEURS Hindi test set.\n",
        "\n",
        "**Outputs:**\n",
        "- `predictions.csv` — Reference vs. prediction pairs\n",
        "- `baseline_metrics.json` — WER/CER/substitution/insertion/deletion rates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch transformers datasets evaluate jiwer librosa soundfile tqdm pandas numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"openai/whisper-small\"\n",
        "LANGUAGE = \"hi\"\n",
        "FLEURS_DATASET = \"google/fleurs\"\n",
        "FLEURS_LANG = \"hi_in\"\n",
        "NUM_EVAL_SAMPLES = 200\n",
        "BATCH_SIZE = 4\n",
        "OUTPUT_DIR = \"outputs/q1\"\n",
        "SEED = 42\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load FLEURS Hindi Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load FLEURS Hindi test split (streaming to save memory)\n",
        "fleurs_test = load_dataset(FLEURS_DATASET, FLEURS_LANG, split=\"test\", trust_remote_code=True)\n",
        "\n",
        "print(f\"Total test samples: {len(fleurs_test)}\")\n",
        "print(f\"Using subset: {min(NUM_EVAL_SAMPLES, len(fleurs_test))} samples\")\n",
        "print(f\"Sample features: {fleurs_test.features}\")\n",
        "\n",
        "# Select subset\n",
        "eval_subset = fleurs_test.select(range(min(NUM_EVAL_SAMPLES, len(fleurs_test))))\n",
        "print(f\"\\nSubset size: {len(eval_subset)}\")\n",
        "print(f\"Sample transcription: {eval_subset[0]['transcription'][:100]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Whisper Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "# Load processor and model\n",
        "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Move to GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Set forced decoder IDs for Hindi\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=LANGUAGE, task=\"transcribe\")\n",
        "\n",
        "print(f\"Model loaded on: {device}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import librosa\n",
        "\n",
        "references = []\n",
        "predictions = []\n",
        "audio_ids = []\n",
        "\n",
        "print(f\"Running inference on {len(eval_subset)} samples...\")\n",
        "\n",
        "for i, sample in enumerate(tqdm(eval_subset, desc=\"Evaluating\")):\n",
        "    # Get audio and reference\n",
        "    audio = sample[\"audio\"][\"array\"]\n",
        "    sr = sample[\"audio\"][\"sampling_rate\"]\n",
        "    reference = sample[\"transcription\"]\n",
        "    \n",
        "    # Resample to 16kHz if needed\n",
        "    if sr != 16000:\n",
        "        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
        "    \n",
        "    # Process audio\n",
        "    input_features = processor(\n",
        "        audio, sampling_rate=16000, return_tensors=\"pt\"\n",
        "    ).input_features.to(device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        predicted_ids = model.generate(\n",
        "            input_features,\n",
        "            forced_decoder_ids=forced_decoder_ids,\n",
        "            max_new_tokens=225,\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    prediction = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "    \n",
        "    references.append(reference)\n",
        "    predictions.append(prediction)\n",
        "    audio_ids.append(sample.get(\"id\", i))\n",
        "\n",
        "print(f\"\\nInference complete. {len(predictions)} predictions generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compute Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jiwer\n",
        "\n",
        "# Corpus-level WER\n",
        "corpus_wer = jiwer.wer(references, predictions)\n",
        "corpus_cer = jiwer.cer(references, predictions)\n",
        "\n",
        "# Detailed breakdown\n",
        "wer_output = jiwer.process_words(references, predictions)\n",
        "total_ref = wer_output.substitutions + wer_output.deletions + wer_output.hits\n",
        "\n",
        "metrics = {\n",
        "    \"label\": \"whisper-small-pretrained\",\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"eval_dataset\": f\"{FLEURS_DATASET}/{FLEURS_LANG}\",\n",
        "    \"num_samples\": len(references),\n",
        "    \"corpus_wer\": round(corpus_wer, 4),\n",
        "    \"corpus_cer\": round(corpus_cer, 4),\n",
        "    \"total_substitutions\": wer_output.substitutions,\n",
        "    \"total_insertions\": wer_output.insertions,\n",
        "    \"total_deletions\": wer_output.deletions,\n",
        "    \"total_hits\": wer_output.hits,\n",
        "    \"total_ref_words\": total_ref,\n",
        "    \"substitution_rate\": round(wer_output.substitutions / max(total_ref, 1), 4),\n",
        "    \"insertion_rate\": round(wer_output.insertions / max(total_ref, 1), 4),\n",
        "    \"deletion_rate\": round(wer_output.deletions / max(total_ref, 1), 4),\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"device\": device,\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BASELINE EVALUATION RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Samples: {metrics['num_samples']}\")\n",
        "print(f\"WER: {metrics['corpus_wer']:.4f}\")\n",
        "print(f\"CER: {metrics['corpus_cer']:.4f}\")\n",
        "print(f\"Substitution Rate: {metrics['substitution_rate']:.4f}\")\n",
        "print(f\"Insertion Rate: {metrics['insertion_rate']:.4f}\")\n",
        "print(f\"Deletion Rate: {metrics['deletion_rate']:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save predictions CSV\n",
        "pred_df = pd.DataFrame({\n",
        "    \"id\": audio_ids,\n",
        "    \"reference\": references,\n",
        "    \"prediction\": predictions,\n",
        "})\n",
        "pred_df.to_csv(os.path.join(OUTPUT_DIR, \"predictions.csv\"), index=False)\n",
        "print(f\"Predictions saved to {OUTPUT_DIR}/predictions.csv\")\n",
        "\n",
        "# Save metrics JSON\n",
        "with open(os.path.join(OUTPUT_DIR, \"baseline_metrics.json\"), \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
        "print(f\"Metrics saved to {OUTPUT_DIR}/baseline_metrics.json\")\n",
        "\n",
        "# Display sample predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(\"-\"*60)\n",
        "for i in range(min(5, len(references))):\n",
        "    print(f\"REF: {references[i][:80]}\")\n",
        "    print(f\"HYP: {predictions[i][:80]}\")\n",
        "    print(\"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Quick Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-utterance WER distribution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "per_utt_wer = []\n",
        "for ref, hyp in zip(references, predictions):\n",
        "    if ref.strip():\n",
        "        per_utt_wer.append(jiwer.wer(ref, hyp))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# WER histogram\n",
        "axes[0].hist(per_utt_wer, bins=30, color='#4A90D9', edgecolor='white', alpha=0.85)\n",
        "axes[0].axvline(np.mean(per_utt_wer), color='red', linestyle='--', label=f'Mean: {np.mean(per_utt_wer):.3f}')\n",
        "axes[0].set_xlabel('WER per utterance')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_title('WER Distribution (Baseline)', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Error type breakdown\n",
        "error_types = ['Substitutions', 'Insertions', 'Deletions']\n",
        "error_counts = [metrics['total_substitutions'], metrics['total_insertions'], metrics['total_deletions']]\n",
        "colors = ['#E8636F', '#F5A623', '#4A90D9']\n",
        "axes[1].bar(error_types, error_counts, color=colors, edgecolor='white')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].set_title('Error Type Breakdown', fontweight='bold')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(error_counts):\n",
        "    axes[1].text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'baseline_analysis.png'), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Analysis plot saved.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
