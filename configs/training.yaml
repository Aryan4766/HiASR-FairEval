# ==============================================================================
# Training Configuration
# ==============================================================================

model:
  name: "openai/whisper-small"
  language: "hi"
  task: "transcribe"

training:
  epochs: 2
  batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 1.0e-5
  warmup_steps: 50
  weight_decay: 0.01
  fp16: true
  max_steps: -1                # -1 = use epochs instead
  eval_steps: 100
  save_steps: 100
  logging_steps: 25
  early_stopping_patience: 3

evaluation:
  dataset: "google/fleurs"
  split: "test"
  language: "hi_in"
  batch_size: 4
  num_samples: 200             # Subset for evaluation

ablation:
  experiments:
    - name: "baseline_lr1e5"
      text_norm: true
      learning_rate: 1.0e-5
    - name: "no_norm_lr1e5"
      text_norm: false
      learning_rate: 1.0e-5
    - name: "baseline_lr5e5"
      text_norm: true
      learning_rate: 5.0e-5

paths:
  output_dir: "outputs/q1"
  model_dir: "outputs/q1/finetuned_model"
  logs_dir: "outputs/q1/logs"

seed: 42
